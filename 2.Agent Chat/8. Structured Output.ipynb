{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Labrador is a breed of Dog\"\n",
    "\n",
    "{\n",
    "    'content':'Dog',\n",
    "    'species':'Labra'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class PlanetInfo(BaseModel):\n",
    "    name: str\n",
    "    color: str\n",
    "    distance_miles: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:413: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "open_router_model_client =  OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    api_key = key,\n",
    "    model_info={\n",
    "        \"family\":'deepseek',\n",
    "        \"vision\" :True,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AssistantAgent(\n",
    "    name='planet_agent',\n",
    "    model_client=open_router_model_client,\n",
    "    system_message=\"You are a helpful assistant that provides information about planets. in the structure JSON\" \\\n",
    "    \"{ name :str\" \\\n",
    "    \"age : int\" \\\n",
    "    \"}\",\n",
    "    output_content_type = PlanetInfo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'structured_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     structured_response \u001b[38;5;241m=\u001b[39m (result\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(structured_response)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m test_structured_output()\n",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36mtest_structured_output\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_structured_output\u001b[39m():\n\u001b[0;32m      2\u001b[0m     task \u001b[38;5;241m=\u001b[39m TextMessage(content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide information about Mars.\u001b[39m\u001b[38;5;124m\"\u001b[39m,source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mrun(task\u001b[38;5;241m=\u001b[39mtask)\n\u001b[0;32m      4\u001b[0m     structured_response \u001b[38;5;241m=\u001b[39m (result\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(structured_response)\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:136\u001b[0m, in \u001b[0;36mBaseChatAgent.run\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages(input_messages, cancellation_token)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39minner_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     output_messages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39minner_messages\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:782\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: Sequence[BaseChatMessage], cancellation_token: CancellationToken) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages_stream(messages, cancellation_token):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[0;32m    784\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:827\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# STEP 3: Run the first inference\u001b[39;00m\n\u001b[0;32m    826\u001b[0m model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm(\n\u001b[0;32m    828\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mmodel_client,\n\u001b[0;32m    829\u001b[0m     model_client_stream\u001b[38;5;241m=\u001b[39mmodel_client_stream,\n\u001b[0;32m    830\u001b[0m     system_messages\u001b[38;5;241m=\u001b[39msystem_messages,\n\u001b[0;32m    831\u001b[0m     model_context\u001b[38;5;241m=\u001b[39mmodel_context,\n\u001b[0;32m    832\u001b[0m     workbench\u001b[38;5;241m=\u001b[39mworkbench,\n\u001b[0;32m    833\u001b[0m     handoff_tools\u001b[38;5;241m=\u001b[39mhandoff_tools,\n\u001b[0;32m    834\u001b[0m     agent_name\u001b[38;5;241m=\u001b[39magent_name,\n\u001b[0;32m    835\u001b[0m     cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    836\u001b[0m     output_content_type\u001b[38;5;241m=\u001b[39moutput_content_type,\n\u001b[0;32m    837\u001b[0m ):\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[0;32m    839\u001b[0m         model_result \u001b[38;5;241m=\u001b[39m inference_output\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:955\u001b[0m, in \u001b[0;36mAssistantAgent._call_llm\u001b[1;34m(cls, model_client, model_client_stream, system_messages, model_context, workbench, handoff_tools, agent_name, cancellation_token, output_content_type)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    956\u001b[0m         llm_messages,\n\u001b[0;32m    957\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    958\u001b[0m         cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    959\u001b[0m         json_output\u001b[38;5;241m=\u001b[39moutput_content_type,\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:594\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create\u001b[1;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    587\u001b[0m     messages: Sequence[LLMMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     cancellation_token: Optional[CancellationToken] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CreateResult:\n\u001b[1;32m--> 594\u001b[0m     create_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_create_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_create_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m     future: Union[Task[ParsedChatCompletion[BaseModel]], Task[ChatCompletion]]\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_params\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;66;03m# Use beta client if response_format is not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:495\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient._process_create_args\u001b[1;34m(self, messages, tools, json_output, extra_create_args)\u001b[0m\n\u001b[0;32m    493\u001b[0m     create_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ResponseFormatText(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_output, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(json_output, BaseModel):\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstructured_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel does not support structured output.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_format_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'structured_output'"
     ]
    }
   ],
   "source": [
    "async def test_structured_output():\n",
    "    task = TextMessage(content = \"Please provide information about Mars.\",source='user')\n",
    "    result = await agent.run(task=task)\n",
    "    structured_response = (result.messages[-1].content)\n",
    "    print(structured_response)\n",
    "\n",
    "await test_structured_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "I am happy.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'structured_output'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create an agent that uses the OpenAI GPT-4o model.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m agent \u001b[38;5;241m=\u001b[39m AssistantAgent(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mopen_router_model_client,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     output_content_type\u001b[38;5;241m=\u001b[39mAgentResponse,\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 21\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(agent\u001b[38;5;241m.\u001b[39mrun_stream(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am happy.\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Check the last message in the result, validate its type, and print the thoughts and response.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], StructuredMessage)\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[0m, in \u001b[0;36mConsole\u001b[1;34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[0m\n\u001b[0;32m    113\u001b[0m last_processed: Optional[T] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    115\u001b[0m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[0;32m    119\u001b[0m         duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py:175\u001b[0m, in \u001b[0;36mBaseChatAgent.run_stream\u001b[1;34m(self, task, cancellation_token)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid message type in sequence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_messages_stream(input_messages, cancellation_token):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, Response):\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m message\u001b[38;5;241m.\u001b[39mchat_message\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:827\u001b[0m, in \u001b[0;36mAssistantAgent.on_messages_stream\u001b[1;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;66;03m# STEP 3: Run the first inference\u001b[39;00m\n\u001b[0;32m    826\u001b[0m model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m inference_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm(\n\u001b[0;32m    828\u001b[0m     model_client\u001b[38;5;241m=\u001b[39mmodel_client,\n\u001b[0;32m    829\u001b[0m     model_client_stream\u001b[38;5;241m=\u001b[39mmodel_client_stream,\n\u001b[0;32m    830\u001b[0m     system_messages\u001b[38;5;241m=\u001b[39msystem_messages,\n\u001b[0;32m    831\u001b[0m     model_context\u001b[38;5;241m=\u001b[39mmodel_context,\n\u001b[0;32m    832\u001b[0m     workbench\u001b[38;5;241m=\u001b[39mworkbench,\n\u001b[0;32m    833\u001b[0m     handoff_tools\u001b[38;5;241m=\u001b[39mhandoff_tools,\n\u001b[0;32m    834\u001b[0m     agent_name\u001b[38;5;241m=\u001b[39magent_name,\n\u001b[0;32m    835\u001b[0m     cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    836\u001b[0m     output_content_type\u001b[38;5;241m=\u001b[39moutput_content_type,\n\u001b[0;32m    837\u001b[0m ):\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inference_output, CreateResult):\n\u001b[0;32m    839\u001b[0m         model_result \u001b[38;5;241m=\u001b[39m inference_output\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:955\u001b[0m, in \u001b[0;36mAssistantAgent._call_llm\u001b[1;34m(cls, model_client, model_client_stream, system_messages, model_context, workbench, handoff_tools, agent_name, cancellation_token, output_content_type)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 955\u001b[0m     model_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m model_client\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m    956\u001b[0m         llm_messages,\n\u001b[0;32m    957\u001b[0m         tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[0;32m    958\u001b[0m         cancellation_token\u001b[38;5;241m=\u001b[39mcancellation_token,\n\u001b[0;32m    959\u001b[0m         json_output\u001b[38;5;241m=\u001b[39moutput_content_type,\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m model_result\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:594\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient.create\u001b[1;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    587\u001b[0m     messages: Sequence[LLMMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    592\u001b[0m     cancellation_token: Optional[CancellationToken] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CreateResult:\n\u001b[1;32m--> 594\u001b[0m     create_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_create_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_create_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m     future: Union[Task[ParsedChatCompletion[BaseModel]], Task[ChatCompletion]]\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m create_params\u001b[38;5;241m.\u001b[39mresponse_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;66;03m# Use beta client if response_format is not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shubu\\Desktop\\AutoGen Hardcode\\venv\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py:495\u001b[0m, in \u001b[0;36mBaseOpenAIChatCompletionClient._process_create_args\u001b[1;34m(self, messages, tools, json_output, extra_create_args)\u001b[0m\n\u001b[0;32m    493\u001b[0m     create_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ResponseFormatText(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(json_output, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(json_output, BaseModel):\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstructured_output\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel does not support structured output.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_format_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'structured_output'"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from autogen_agentchat.ui import Console\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "# The response format for the agent as a Pydantic base model.\n",
    "class AgentResponse(BaseModel):\n",
    "    thoughts: str\n",
    "    response: Literal[\"happy\", \"sad\", \"neutral\"]\n",
    "\n",
    "\n",
    "# Create an agent that uses the OpenAI GPT-4o model.\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    model_client=open_router_model_client,\n",
    "    system_message=\"Categorize the input as happy, sad, or neutral following the JSON format.\",\n",
    "    # Define the output content type of the agent.\n",
    "    output_content_type=AgentResponse,\n",
    ")\n",
    "\n",
    "result = await Console(agent.run_stream(task=\"I am happy.\"))\n",
    "\n",
    "# Check the last message in the result, validate its type, and print the thoughts and response.\n",
    "assert isinstance(result.messages[-1], StructuredMessage)\n",
    "assert isinstance(result.messages[-1].content, AgentResponse)\n",
    "print(\"Thought: \", result.messages[-1].content.thoughts)\n",
    "print(\"Response: \", result.messages[-1].content.response)\n",
    "await model_client.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
