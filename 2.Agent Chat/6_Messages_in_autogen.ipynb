{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3337e98c",
   "metadata": {},
   "source": [
    "# Messages in Autogen v0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d465c9",
   "metadata": {},
   "source": [
    "We can imaging messages as the way agent communicate - text our Friend. \n",
    "\n",
    "When we communicate with the agents -----> sending a message\n",
    "when it responds ---> it too sends a message\n",
    "\n",
    "TextMessage \n",
    "ImageMessage\n",
    "ToolMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87af358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.messages import TextMessage,MultiModalMessage\n",
    "from autogen_core import Image as AGImage\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPEN_ROUTER_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"OPEN_ROUTER_API_KEY environment variable is not set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a980db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec780236",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_router_model_client =  OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"deepseek/deepseek-chat-v3-0324:free\",\n",
    "    api_key = key,\n",
    "    model_info={\n",
    "        \"family\":'deepseek',\n",
    "        \"vision\" :True,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712e7d5",
   "metadata": {},
   "source": [
    "## Simplest Type of Message - Text Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2ed377",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = AssistantAgent(\n",
    "    name = \"text_agent\",\n",
    "    model_client=open_router_model_client,\n",
    "    system_message=\"you are a helpful assistant answer the questions carefully\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba889bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I‚Äôm DeepSeek Chat, your AI assistant created by DeepSeek. üòä I‚Äôm here to help answer your questions, provide information, and assist with all sorts of tasks‚Äîwhether it‚Äôs learning something new, solving a problem, or just having a fun conversation!  \n",
      "\n",
      "Feel free to ask me anything‚Äîwhat can I help you with today? üöÄ\n"
     ]
    }
   ],
   "source": [
    "async def test_text_message():\n",
    "    text_msg = TextMessage(content = \"who are you?\", source = \"user\") #source can be agent\n",
    "    result = await assistant.run(task=text_msg)\n",
    "    print(result.messages[-1].content)\n",
    "\n",
    "await test_text_message()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b1a82b",
   "metadata": {},
   "source": [
    "## MultiModal Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3604b76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I currently **can‚Äôt view or analyze images directly**. However, if you describe the image in detail (e.g., objects, colors, text, or context), I can help interpret it or answer questions about it!  \n",
      "\n",
      "For example, you could ask:  \n",
      "- *\"What does this diagram about photosynthesis mean?\"*  \n",
      "- *\"Can you explain the symbolism in a painting of X?\"*  \n",
      "- *\"What might a picture of a red apple on a table represent?\"*  \n",
      "\n",
      "Let me know how I can assist! üòä\n"
     ]
    }
   ],
   "source": [
    "import requests                        # For making HTTP requests to fetch the image\n",
    "from io import BytesIO                # To wrap raw bytes in a file-like buffer\n",
    "from PIL import Image                 # Pillow, for decoding image bytes into an Image object \n",
    "                                       # AGImage & MultiModalMessage types from AutoGen\n",
    "import asyncio                         # To run our async test function\n",
    "\n",
    "async def test_multi_modal():\n",
    "    # 1. Fetch the image bytes from Picsum\n",
    "    response = requests.get('https://picsum.photos/id/237/200/300')\n",
    "    #    ‚Ä¢ response.content is the raw JPEG/PNG bytes\n",
    "    \n",
    "    # 2. Decode the raw bytes into a PIL image\n",
    "    pil_image = Image.open(BytesIO(response.content))\n",
    "    #    ‚Ä¢ BytesIO wraps bytes in a file-like buffer\n",
    "    #    ‚Ä¢ Image.open() reads from that buffer to produce a PIL.Image.Image\n",
    "    \n",
    "    # 3. Wrap the PIL image in AutoGen‚Äôs multimodal image type\n",
    "    ag_image = AGImage(pil_image)\n",
    "    #    ‚Ä¢ AGImage tells the agent ‚Äúthis is image data‚Äù in a message\n",
    "    \n",
    "    # 4. Build a multimodal user message: text prompt + image\n",
    "    multi_modal_msg = MultiModalMessage(\n",
    "        content=['What is in the image?', ag_image],\n",
    "        source='user'\n",
    "    )\n",
    "    #    ‚Ä¢ content list order matters: first the question, then the image\n",
    "    #    ‚Ä¢ source='user' tags this as coming from the user side\n",
    "    \n",
    "    # 5. Send the multimodal message to the agent and await the reply\n",
    "    result = await assistant.run(task=multi_modal_msg)\n",
    "    #    ‚Ä¢ agent.run() processes text+image together and returns a chat history\n",
    "    \n",
    "    # 6. Print out the assistant‚Äôs final reply\n",
    "    print(result.messages[-1].content)\n",
    "    #    ‚Ä¢ result.messages[-1] is the assistant‚Äôs response message\n",
    "    #    ‚Ä¢ .content is the text of that reply\n",
    "\n",
    "# 7. Actually invoke the async test function\n",
    "await test_multi_modal()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
